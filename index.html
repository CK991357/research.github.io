<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Note</title>
    <style>
        body {
            font-family: sans-serif;
            margin: 20px;
        }
        h1 {
            text-align: center;
        }
        ul {
            list-style: none;
            padding: 0;
        }
        li {
            margin-bottom: 20px;
            border-bottom: 1px solid #ccc;
            padding-bottom: 10px;
        }
        .title {
            font-weight: bold;
        }
        .date {
            font-style: italic;
            color: #666;
        }
    </style>
</head>
<body>
    <h1>Research Note</h1>

    <ul>
        <li>
            <a href="Transformers_without_Normalization.html" class="title">Transformers without Normalization</a><br>
            <span class="date">2025-03-16</span><br>
            This article challenges the conventional wisdom that normalization layers are indispensable in modern neural network training, proposing Dynamic Tanh (DyT) as a simple alternative to normalization layers in the Transformer architecture.
        </li>
        <!-- Add more articles here -->
    </ul>
</body>
</html>
